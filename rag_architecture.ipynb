{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI,OpenAIEmbeddings\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key=os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step by step implementation of each component of RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_embeddings(data_path: str, embedding_model) -> tuple:\n",
    "    \"\"\"\n",
    "    Generates vector embeddings for a given dataset using the specified embedding model.\n",
    "    It combines the 'Title' and 'Text' columns before generating embeddings.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): The file path to the CSV dataset containing 'Title' and 'Text' columns.\n",
    "        embedding_model: An embedding model instance with a method `embed_query` that converts text into a vector.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - data (pd.DataFrame): The original dataset with an added 'Vectors' column containing embeddings.\n",
    "            - vector_list (np.ndarray): A NumPy array of vector embeddings corresponding to the combined 'Title' and 'Text' data.\n",
    "\n",
    "    Example:\n",
    "        >>> from some_embedding_library import EmbeddingModel\n",
    "        >>> model = EmbeddingModel()\n",
    "        >>> docs, vecs = create_embeddings(\"data.csv\", model)\n",
    "    \"\"\"\n",
    "    \n",
    "    data = pd.read_csv(data_path)\n",
    "\n",
    "    # Combine \"Title\" and \"Text\" columns\n",
    "    text_data = (data[\"Title\"] + \" \" + data[\"Text\"]).tolist()\n",
    "\n",
    "    # Generate embeddings for combined text\n",
    "    vector_list = np.array([embedding_model.embed_query(text) for text in text_data])\n",
    "\n",
    "    # Store vectors in the DataFrame\n",
    "    data['Vectors'] = vector_list\n",
    "\n",
    "    return data, vector_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create vector db with fiass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "def build_faiss_index(vector_emb: np.ndarray) -> faiss.IndexFlatL2:\n",
    "    \"\"\"\n",
    "    Creates a FAISS index and adds vector embeddings to it.\n",
    "\n",
    "    Args:\n",
    "        vector_emb (np.ndarray): NumPy array of vector embeddings.\n",
    "\n",
    "    Returns:\n",
    "        faiss.IndexFlatL2: The FAISS index with stored embeddings.\n",
    "    \"\"\"\n",
    "    embedding_dim = vector_emb.shape[1]  # Get the embedding dimension\n",
    "    index = faiss.IndexFlatL2(embedding_dim)  # Create a FAISS index using L2 distance\n",
    "    index.add(vector_emb)  # Add vectors to the index\n",
    "    return index\n",
    "\n",
    "def create_faiss_vector_store(index: faiss.IndexFlatL2, df, embedding_function) -> FAISS:\n",
    "    \"\"\"\n",
    "    Creates a FAISS vector store by mapping document IDs to the FAISS index.\n",
    "\n",
    "    Args:\n",
    "        index (faiss.IndexFlatL2): The FAISS index with stored embeddings.\n",
    "        df (pd.DataFrame): The DataFrame containing document data.\n",
    "        embedding_function: The embedding function used for vectorization.\n",
    "\n",
    "    Returns:\n",
    "        FAISS: A FAISS vector store instance for retrieval.\n",
    "    \"\"\"\n",
    "    docstore = InMemoryDocstore()  # In-memory document storage\n",
    "    index_to_docstore_id = {i: str(df.loc[i, \"ID\"]) for i in range(len(df))}  # Mapping\n",
    "\n",
    "    vector_store = FAISS(\n",
    "        embedding_function=embedding_function,\n",
    "        index=index,\n",
    "        docstore=docstore,\n",
    "        index_to_docstore_id=index_to_docstore_id,\n",
    "    )\n",
    "\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import os\n",
    "\n",
    "def save_faiss_index(index: faiss.IndexFlatL2, file_path: str):\n",
    "    \"\"\"\n",
    "    Saves the FAISS index to disk.\n",
    "    Args:\n",
    "        index (faiss.IndexFlatL2): The FAISS index to be saved.\n",
    "        file_path (str): The file path where the index will be stored.\n",
    "    \"\"\"\n",
    "    faiss.write_index(index, file_path)\n",
    "    print(f\"FAISS index saved to {file_path}\")\n",
    "    \n",
    "\n",
    "def load_faiss_index(file_path: str) -> faiss.IndexFlatL2:\n",
    "    \"\"\"\n",
    "    Loads a FAISS index from disk.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The file path of the saved FAISS index.\n",
    "\n",
    "    Returns:\n",
    "        faiss.IndexFlatL2: The loaded FAISS index.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"No FAISS index found at {file_path}\")\n",
    "    \n",
    "    index = faiss.read_index(file_path)\n",
    "    print(f\"FAISS index loaded from {file_path}\")\n",
    "    return index\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def save_index_to_docstore_mapping(index_to_docstore_id, file_path):\n",
    "    \"\"\"\n",
    "    Save the index to document mapping to a JSON file.\n",
    "\n",
    "    Args:\n",
    "    - index_to_docstore_id (dict): A dictionary mapping index to document store ID.\n",
    "    - file_path (str): The path where the mapping will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(index_to_docstore_id, f)\n",
    "        print(f\"Mapping successfully saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving mapping: {e}\")\n",
    "\n",
    "\n",
    "def load_index_to_docstore_mapping(file_path):\n",
    "    \"\"\"\n",
    "    Load the index to document mapping from a JSON file.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The path from which the mapping will be loaded.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary mapping index to document store ID.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            index_to_docstore_id = json.load(f)\n",
    "        print(f\"Mapping successfully loaded from {file_path}\")\n",
    "        return index_to_docstore_id\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading mapping: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query, data,  index, index_to_docstore_id, top_k=3):\n",
    "    \"\"\"Convert user query to embedding, retrieve top_k relevant documents.\"\"\"\n",
    "    query_vector = np.array(embeddings.embed_query(query))  # Convert query to vector\n",
    "    distances, indices = index.search(np.array([query_vector]), top_k)  # Search in FAISS\n",
    "\n",
    "    retrieved_docs = []\n",
    "    for idx in indices[0]:  # Extract the top_k retrieved document indices\n",
    "        doc_id = index_to_docstore_id[idx]\n",
    "        doc = data[data[\"ID\"] == int(doc_id)].iloc[0]\n",
    "        retrieved_docs.append(f\"Title: {doc['Title']}\\nText: {doc['Text']}\")\n",
    "    return retrieved_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize OpenAI model\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "def generate_response_with_chain(query, data,  index, index_to_docstore_id, top_k=3):\n",
    "    \"\"\"Retrieve relevant documents and use LangChain to generate a response.\"\"\"\n",
    "\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = retrieve_documents(query,data,  index, index_to_docstore_id, top_k)\n",
    "    context = \"\\n\\n\".join(retrieved_docs)  # Format context\n",
    "\n",
    "    # Define a ChatPromptTemplate\n",
    "    prompt_template = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are an expert assistant providing detailed and accurate responses.\n",
    "        \n",
    "        Here is some relevant information:\n",
    "\n",
    "        {context}\n",
    "\n",
    "        Now, answer the following question:\n",
    "\n",
    "        {query}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Create an LLMChain\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Run the chain\n",
    "    response = chain.run(context=context, query=query)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# full system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RAGSystem:\n",
    "    def __init__(self, embedding_model, data_path, vectore_store_folder, index_file_name, docstore_to_id_file_name, llm_model=\"gpt-3.5-turbo\"):\n",
    "        \"\"\"\n",
    "        Initializes the RAG system with an embedding model and an LLM.\n",
    "\n",
    "        Args:\n",
    "            embedding_model: The embedding function to convert text into vectors.\n",
    "            llm_model (str): The OpenAI model to use for generating responses.\n",
    "        \"\"\"\n",
    "        self.embedding_model = embedding_model\n",
    "        self.llm = ChatOpenAI(model=llm_model)\n",
    "        self.data_path =  data_path\n",
    "        self.data = None\n",
    "        self.vector_store = None\n",
    "        self.index = None\n",
    "        self.index_to_docstore_id  = None\n",
    "        self.vectore_store_folder = vectore_store_folder\n",
    "        os.makedirs(self.vectore_store_folder, exist_ok=True)\n",
    "        self.mapping_full_path= os.path.join(self.vectore_store_folder, docstore_to_id_file_name)\n",
    "        self.index_full_path = os.path.join(self.vectore_store_folder, index_file_name)\n",
    "      \n",
    "        self.initialize_docstore()\n",
    "\n",
    "\n",
    "    \n",
    "    def create_embeddings(self,save_emb = False) -> tuple:\n",
    "        \"\"\"\n",
    "        Generates vector embeddings for a given dataset using the specified embedding model.\n",
    "        It combines the 'Title' and 'Text' columns before generating embeddings.\n",
    "\n",
    "        Args:\n",
    "            data_path (str): The file path to the CSV dataset containing 'Title' and 'Text' columns.\n",
    "            embedding_model: An embedding model instance with a method `embed_query` that converts text into a vector.\n",
    "        \"\"\"\n",
    "        \n",
    "        data = pd.read_csv(self.data_path)\n",
    "\n",
    "        # Combine \"Title\" and \"Text\" columns\n",
    "        text_data = (data[\"Title\"] + \" \" + data[\"Text\"]).tolist()\n",
    "        # Generate embeddings for combined text\n",
    "        vector_list = np.array([self.embedding_model.embed_query(text) for text in text_data])\n",
    "        # Store vectors in the DataFrame\n",
    "        data['Vectors'] = list(vector_list)\n",
    "        self.data = data\n",
    "        if save_emb:\n",
    "            data.to_csv(\"data_with_embeddings.csv\", index=False)\n",
    "        return data, vector_list\n",
    "    \n",
    "\n",
    "    def build_faiss_index(self, vector_emb: np.ndarray) -> faiss.IndexFlatL2:\n",
    "        \"\"\"\n",
    "        Creates a FAISS index and adds vector embeddings to it.\n",
    "\n",
    "        Args:\n",
    "            vector_emb (np.ndarray): NumPy array of vector embeddings. \n",
    "        \"\"\"\n",
    "        embedding_dim = vector_emb.shape[1]  # Get the embedding dimension\n",
    "        index = faiss.IndexFlatL2(embedding_dim)  # Create a FAISS index using L2 distance\n",
    "        index.add(vector_emb)  # Add vectors to the index\n",
    "        return index\n",
    "    \n",
    "    def get_index_to_docstore_id_mapping(self):\n",
    "        \"\"\"\n",
    "        Retrieves or creates a mapping between FAISS index IDs and document store IDs.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary mapping FAISS index IDs to document store IDs.\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.mapping_full_path):\n",
    "            index_to_docstore_id = self.load_index_to_docstore_mapping(self.mapping_full_path)\n",
    "        else:\n",
    "            index_to_docstore_id = {i: str(self.data.loc[i, \"ID\"]) for i in range(len(self.data))}  # Mapping\n",
    "            self.index_to_docstore_id = index_to_docstore_id\n",
    "            self.save_index_to_docstore_mapping(self.index_to_docstore_id, self.mapping_full_path)\n",
    "        return index_to_docstore_id\n",
    "    \n",
    "    def get_faiss_index(self):\n",
    "        \"\"\"\n",
    "        Retrieves or creates a FAISS index.\n",
    "\n",
    "        Returns:\n",
    "            faiss.IndexFlatL2: The FAISS index.\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.index_full_path):\n",
    "            index = self.load_faiss_index(self.index_full_path)\n",
    "        else:   \n",
    "            data, embeddings = self.create_embeddings()\n",
    "            index = self.build_faiss_index(embeddings)\n",
    "            self.save_faiss_index(index, self.index_full_path)\n",
    "\n",
    "        return index\n",
    "\n",
    "    def save_faiss_index(self,index: faiss.IndexFlatL2, file_path):\n",
    "        \"\"\"\n",
    "        Saves the FAISS index to disk.\n",
    "        Args:\n",
    "            index (faiss.IndexFlatL2): The FAISS index to be saved.\n",
    "            file_path (str): The file path where the index will be stored.\n",
    "        \"\"\"\n",
    "        faiss.write_index(index, file_path)\n",
    "        print(f\"FAISS index saved to {file_path}\")\n",
    "        \n",
    "\n",
    "    def load_faiss_index(self, file_path: str) -> faiss.IndexFlatL2:\n",
    "        \"\"\"\n",
    "        Loads a FAISS index from disk.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The file path of the saved FAISS index.\n",
    "\n",
    "        Returns:\n",
    "            faiss.IndexFlatL2: The loaded FAISS index.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"No FAISS index found at {file_path}\")\n",
    "        \n",
    "        index = faiss.read_index(file_path)\n",
    "        print(f\"FAISS index loaded from {file_path}\")\n",
    "        return index\n",
    "\n",
    "    def save_index_to_docstore_mapping(self,index_to_docstore_id, file_path):\n",
    "        \"\"\"\n",
    "        Save the index to document mapping to a JSON file.\n",
    "\n",
    "        Args:\n",
    "        - index_to_docstore_id (dict): A dictionary mapping index to document store ID.\n",
    "        - file_path (str): The path where the mapping will be saved.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'w') as f:\n",
    "                json.dump(index_to_docstore_id, f)\n",
    "            print(f\"Mapping successfully saved to {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving mapping: {e}\")\n",
    "\n",
    "\n",
    "    def load_index_to_docstore_mapping(self, file_path):\n",
    "        \"\"\"\n",
    "        Load the index to document mapping from a JSON file.\n",
    "\n",
    "        Args:\n",
    "        - file_path (str): The path from which the mapping will be loaded.\n",
    "\n",
    "        Returns:\n",
    "        - dict: A dictionary mapping index to document store ID.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                index_to_docstore_id = json.load(f)\n",
    "            print(f\"Mapping successfully loaded from {file_path}\")\n",
    "            return index_to_docstore_id\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading mapping: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_faiss_vector_store(self,index: faiss.IndexFlatL2, df, embedding_function, index_to_docstore_id) -> FAISS:\n",
    "        \"\"\"\n",
    "        Creates a FAISS vector store by mapping document IDs to the FAISS index.\n",
    "\n",
    "        Args:\n",
    "            index (faiss.IndexFlatL2): The FAISS index with stored embeddings.\n",
    "            df (pd.DataFrame): The DataFrame containing document data.\n",
    "            embedding_function: The embedding function used for vectorization.\n",
    "\n",
    "        \"\"\"\n",
    "        docstore = InMemoryDocstore()  # In-memory document \n",
    "        \n",
    "        vector_store = FAISS(\n",
    "            embedding_function=embedding_function,\n",
    "            index=index,\n",
    "            docstore=docstore,\n",
    "            index_to_docstore_id=index_to_docstore_id,\n",
    "        )\n",
    "        return vector_store\n",
    "    \n",
    "\n",
    "    def retrieve_documents(self, query, top_k=3):\n",
    "        \"\"\"\n",
    "        Converts a query into an embedding and retrieves the top_k most relevant documents.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user query.\n",
    "            top_k (int): Number of documents to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            List of relevant document texts.\n",
    "        \"\"\"\n",
    "        if self.index is None or self.index_to_docstore_id is None:\n",
    "            raise ValueError(\"FAISS index or mapping not initialized.\")\n",
    "\n",
    "        query_vector = np.array(self.embedding_model.embed_query(query))  # Convert query to vector\n",
    "        distances, indices = self.index.search(np.array([query_vector]), top_k)\n",
    "\n",
    "        retrieved_docs = []\n",
    "        for idx in indices[0]:  \n",
    "            doc_id = self.index_to_docstore_id[str(idx)]\n",
    "            doc = self.data[self.data[\"ID\"] == int(doc_id)].iloc[0]\n",
    "            retrieved_docs.append(f\"Title: {doc['Title']}\\nText: {doc['Text']}\")\n",
    "\n",
    "        return retrieved_docs\n",
    "    \n",
    "    def generate_response(self, query, top_k=3):\n",
    "        \"\"\"\n",
    "        Retrieves relevant documents and generates a response using the LLM.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user query.\n",
    "            top_k (int): Number of documents to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated response.\n",
    "        \"\"\"\n",
    "        retrieved_docs = self.retrieve_documents(query, top_k)\n",
    "        context = \"\\n\\n\".join(retrieved_docs)\n",
    "    # Define the prompt template\n",
    "        prompt_template = ChatPromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            You are an expert assistant providing detailed and accurate responses based on provided context.\n",
    "            The answers must be concrete and strictly refer to the question. Don't add anything outside the \n",
    "            knowledge base.\n",
    "\n",
    "            Here is some relevant information:\n",
    "\n",
    "            {context}\n",
    "\n",
    "            Now, answer the following question:\n",
    "\n",
    "            {query}\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        # Create the chain\n",
    "        chain = prompt_template | self.llm | StrOutputParser()\n",
    "\n",
    "        # Invoke the chain with the input\n",
    "        response = chain.invoke({\"context\": context, \"query\": query})\n",
    "        return response\n",
    "    \n",
    "    def initialize_docstore(self, save_emb=False):\n",
    "        if not self.vector_store or  not self.index_to_docstore_id or not self.index:\n",
    "            if not self.vector_store:\n",
    "                _, vector_list = self.create_embeddings(save_emb=save_emb)\n",
    "                self.vectore_store = self.create_faiss_vector_store(self.index, self.data, self.embedding_model, self.index_to_docstore_id)\n",
    "            if not self.index:\n",
    "                self.index = self.get_faiss_index()\n",
    "            if not self.index_to_docstore_id:\n",
    "                self.index_to_docstore_id  = self.get_index_to_docstore_id_mapping()\n",
    "\n",
    "\n",
    "    def run_pipeline(self,query, save_emb=False, top_k=3):\n",
    "        \"\"\"\n",
    "        Executes the full RAG pipeline, including embedding generation, FAISS index creation, \n",
    "        document retrieval, and response generation.\n",
    "\n",
    "        This method checks if the FAISS index, vector store, and index-to-docstore mapping \n",
    "        are already initialized. If not, it generates embeddings, builds the FAISS index, \n",
    "        and creates the necessary mappings. Finally, it retrieves relevant documents and \n",
    "        generates a response to the user's query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user query for which a response is to be generated.\n",
    "            save_emb (bool, optional): Whether to save the generated embeddings to a CSV file. \n",
    "                                    Defaults to False.\n",
    "            top_k (int, optional): The number of relevant documents to retrieve for the query. \n",
    "                                Defaults to 3.\n",
    "\n",
    "        Returns:\n",
    "            None: The generated response is printed to the console.\n",
    "        \n",
    "        \"\"\"\n",
    "        if not self.vector_store or not self.index_to_docstore_id or not self.index:\n",
    "            self.initialize_docstore()\n",
    "\n",
    "\n",
    "        else:\n",
    "            print('Faiss information is already loaded')\n",
    "        print(self.generate_response(query, top_k=top_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index loaded from faiss_docs\\faiss_index.bin\n",
      "Mapping successfully loaded from faiss_docs\\index_mapping.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_path = 'data.csv'\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "llm_model = \"gpt-3.5-turbo\"\n",
    "vectore_store_folder = 'faiss_docs'\n",
    "mapping_file_name = \"index_mapping.json\"\n",
    "faiss_index_file_name =  \"faiss_index.bin\"\n",
    "\n",
    "rag_system = RAGSystem(embedding_model, data_path, \n",
    "                       vectore_store_folder, faiss_index_file_name, \n",
    "                       mapping_file_name, llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI drug discovery started in the 2020s.\n"
     ]
    }
   ],
   "source": [
    "rag_system.run_pipeline('In what year AI drug discovery started?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factual Recall Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smartphones like the BlackBerry and later the iPhone are credited with combining communication, computing, and internet access in the early 2000s. These devices revolutionized the way people interacted with technology by integrating multiple functions into a single device.\n"
     ]
    }
   ],
   "source": [
    "query = \"Which devices are credited with combining communication, computing, and internet access in the early 2000s?\"\n",
    "rag_system.run_pipeline(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dartmouth Conference in 1956 marked the birth of artificial intelligence as a field of study.\n"
     ]
    }
   ],
   "source": [
    "query = \"What event in 1956 marked the birth of artificial intelligence as a field of study?\"\n",
    "rag_system.run_pipeline(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contextual Understanding Questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The James Webb Space Telescope (JWST) is unique compared to previous space telescopes due to its ability to observe the universe in infrared light. This allows JWST to provide unprecedented insights into the formation of galaxies, stars, and planetary systems. Additionally, JWST is designed to be the most powerful space telescope ever built, surpassing the capabilities of telescopes like the Hubble Space Telescope. Its advanced technology and infrared capabilities make JWST a groundbreaking tool for astronomers and scientists studying the cosmos.\n"
     ]
    }
   ],
   "source": [
    "query = \"What makes the James Webb Space Telescope unique compared to previous space telescopes?\"\n",
    "rag_system.run_pipeline(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthesis and Reasoning Questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The achievement of quantum supremacy in 2019 could potentially influence the future development of artificial intelligence in a few ways:\n",
      "\n",
      "1. Improved AI algorithms: Quantum computers have the potential to significantly speed up computations, allowing for faster training of AI models and the development of more complex algorithms. This could lead to advancements in AI capabilities and performance.\n",
      "\n",
      "2. Quantum machine learning: Quantum computing could be used to enhance machine learning algorithms by leveraging quantum principles such as superposition and entanglement. This could lead to more efficient and powerful AI systems.\n",
      "\n",
      "3. Solving complex problems: Quantum computers have the ability to solve complex problems that are currently beyond the reach of classical computers. This could enable AI systems to tackle more challenging tasks and make breakthroughs in areas such as natural language processing, computer vision, and drug discovery.\n",
      "\n",
      "Overall, the achievement of quantum supremacy could open up new possibilities for the intersection of quantum computing and artificial intelligence, leading to innovative advancements in AI technologies.\n"
     ]
    }
   ],
   "source": [
    "query = \"How might the achievement of quantum supremacy in 2019 influence the future development of artificial intelligence?\"\n",
    "rag_system.run_pipeline(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*here the text and title are used together, but metadata approach can be applied\n",
    "\n",
    "*I used more manual approach for every concept of rag to provide more flexibilty, but it is slower than the\n",
    "Langchain full automotated retrival approaches, these approaches with Recursive Character Splitter can be experimented(but they are less flexible)\n",
    "\n",
    "*multi-query approach can be applied"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
